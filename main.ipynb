{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('AnaliseDeDados': conda)"
  },
  "interpreter": {
   "hash": "38553c9ee6fffc4273e52edb5b2fd383ec338464c02b5c687569d7ad88dc5379"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Reddit Finace Data analysis, \n",
    "## Joao P. Maia\n",
    "\n",
    "Reddit is a major place where people discuss finance and the stock market. What r/Wallstreetbets did no GME in early 2021 is a good example.\n",
    "\n",
    "In this notebook, I'm going to analyze and explore posts from the biggest subreddits(15k>posts)  about finance using tools written in python for data analysis and natural language processing. \n",
    "\n",
    "The dataset was gathered from this [Keagle dataset](https://www.kaggle.com/leukipp/reddit-finance-data). It contains posts from early 2021 until now (05/07/2021).\n",
    "\n",
    "We are using data from those subreddits:\n",
    "- r/wallstreetbets: #638158 (2021-01-01 00:02:06 - 2021-07-05 19:05:13)\n",
    "- r/gme: #224149 (2021-01-01 04:08:51 - 2021-07-05 19:01:50)\n",
    "- r/personalfinance: #64798 (2021-01-24 19:30:31 - 2021-07-05 19:03:36)\n",
    "- r/stocks: #50729 (2021-01-01 00:05:17 - 2021-07-05 19:03:41)\n",
    "- r/pennystocks: #42571 (2021-01-01 00:13:41 - 2021-07-05 19:04:28)\n",
    "- r/stockmarket: #27710 (2021-01-01 02:42:42 - 2021-07-05 18:59:07)\n",
    "- r/investing: #25486 (2021-01-01 00:18:40 - 2021-07-05 19:03:27)\n",
    "- r/robinhoodpennystocks: #21200 (2021-01-01 00:27:36 - 2021-07-05 14:34:27)\n",
    "- r/robinhoodpennystocks: #21200 (2021-01-01 00:27:36 - 2021-07-05 14:34:27)\n",
    "- r/options: #18378 (2021-01-01 01:39:43 - 2021-07-05 19:00:27)\n",
    "\n",
    "\n",
    "---\n",
    "All analyses are made in different notebooks to keep the code organized. Each notebook is about a different term (such as GME, SILVER...). However, this file (main.py) is about cleaning and categorizing the data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading all IDs into a list\n",
    "engine = create_engine(\"sqlite:///db/reddit_financial.db\")\n",
    "dfIDS = pd.read_sql_query(\n",
    "    \"SELECT id FROM reddit_data\",\n",
    "    con=engine,\n",
    ")\n",
    "dfIDS = dfIDS[\"id\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ADDED[./datasets\\gme\\submissions_reddit] - gme\n",
      "ADDED[./datasets\\investing\\submissions_reddit] - investing\n",
      "ADDED[./datasets\\options\\submissions_reddit] - options\n",
      "ADDED[./datasets\\pennystocks\\submissions_reddit] - pennystocks\n",
      "ADDED[./datasets\\personalfinance\\submissions_reddit] - personalfinance\n",
      "ADDED[./datasets\\robinhoodpennystocks\\submissions_reddit] - robinhoodpennystocks\n",
      "ADDED[./datasets\\stockmarket\\submissions_reddit] - stockmarket\n",
      "ADDED[./datasets\\stocks\\submissions_reddit] - stocks\n",
      "ADDED[./datasets\\wallstreetbets\\submissions_reddit] - wallstreetbets\n"
     ]
    }
   ],
   "source": [
    "#Transform all csv files in dataframes\n",
    "dataframes = []\n",
    "\n",
    "for subdir, dirs, files in os.walk(\"./datasets\"):\n",
    "    for filename in files:\n",
    "        filepath = subdir + os.sep + filename\n",
    "\n",
    "        if filepath.endswith(\".csv\"):\n",
    "            print (\"ADDED[{}] - {}\".format(filepath,filepath.split(os.sep)[-2]).replace(\".csv\",\"\"))\n",
    "            df = pd.read_csv(filepath)\n",
    "\n",
    "            #Cleaning deleted/removed or null posts\n",
    "            df = df.dropna(subset=['selftext'])\n",
    "            \n",
    "            df = df[df['deleted'] !=1]\n",
    "            df = df[df['removed'] !=1]\n",
    "\n",
    "            df[\"selftext\"] = df[\"selftext\"].astype(str)\n",
    "\n",
    "            #Remove if ID exists\n",
    "            df = df.query('id not in @dfIDS')\n",
    "\n",
    "\n",
    "            \n",
    "            df['subReddit'] = filepath.split(os.sep)[-2].replace(\".csv\",\"\")\n",
    "            if(len(df)>0):\n",
    "                dataframes.append(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['gme'] : 1\nTotal Size: 1\n"
     ]
    }
   ],
   "source": [
    "tam = 0\n",
    "for df in dataframes:\n",
    "   print(\"{} : {}\".format(df[\"subReddit\"].unique(),len(df)))\n",
    "   tam += len(df)\n",
    "print(\"Total Size: {}\".format(tam))"
   ]
  },
  {
   "source": [
    "## Cleanning\n",
    "\n",
    "The next step is to clear all the data from the bodies/titles to achieve better results in the sentiment analysis.\n",
    "\n",
    "This piece of code was given by the ICA laboratory in PUC-RIO. \n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from contractions import contractions_dict\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\maiaj\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer and stopword\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "tokenizer = ToktokTokenizer()\n",
    "nltk.download('stopwords')\n",
    "stopword_list = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base code:\n",
    "# https://github.com/dipanjanS/practical-machine-learning-with-python/blob/master/notebooks/Ch07_Analyzing_Movie_Reviews_Sentiment/text_normalizer.pys\n",
    "\n",
    "def remove_stopwords(text):\n",
    "  tokens = tokenizer.tokenize(text)\n",
    "  tokens = [token.strip() for token in tokens]\n",
    "  filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "  filtered_text = ' '.join(filtered_tokens)\n",
    "  return filtered_text\n",
    "\n",
    "def lemmatize(text):\n",
    "  text = nlp(text)\n",
    "  text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "  return text\n",
    "\n",
    "def stemmer(text):\n",
    "  ps = nltk.porter.PorterStemmer()\n",
    "  text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "  return text\n",
    "\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "  special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "  text = special_char_pattern.sub(\" \\\\1 \", text)\n",
    "\n",
    "  pattern = r'[^a-zA-z0-9\\s]'\n",
    "  text = re.sub(pattern, '', text)\n",
    "  return text\n",
    "\n",
    "\n",
    "def expand_contractions(text, contraction_mapping=contractions_dict):\n",
    "  contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),\n",
    "                                    flags=re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "  def expand_match(contraction):\n",
    "      match = contraction.group(0)\n",
    "      first_char = match[0]\n",
    "      expanded_contraction = contraction_mapping.get(match) \\\n",
    "          if contraction_mapping.get(match) \\\n",
    "          else contraction_mapping.get(match.lower())\n",
    "      expanded_contraction = first_char + expanded_contraction[1:]\n",
    "      return expanded_contraction\n",
    "  \n",
    "  try:\n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "  except:\n",
    "    return text\n",
    "  return expanded_text\n",
    "\n",
    "def remove_accent(text):\n",
    "  text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "  return text\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    text = re.sub(r\"http[s]?://\\S+\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    stripped_text = soup.get_text()\n",
    "    return stripped_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(corpus):\n",
    "\n",
    "  normalized_corpus = []\n",
    "  for doc in corpus:\n",
    "    doc = strip_html_tags(doc)\n",
    "\n",
    "    doc = remove_accent(doc)\n",
    "\n",
    "    doc = expand_contractions(doc)\n",
    "\n",
    "    doc = doc.lower()\n",
    "\n",
    "    doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ', doc)\n",
    "\n",
    "    doc = lemmatize(doc)\n",
    "\n",
    "    doc = remove_special_characters(doc)\n",
    "\n",
    "    doc = re.sub(' +', ' ', doc)\n",
    "\n",
    "    doc = remove_stopwords(doc)\n",
    "\n",
    "    normalized_corpus.append(doc)\n",
    "\n",
    "  return normalized_corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cleaning Time:\n0.0005333383878072102 min\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "''''''\n",
    "for df in dataframes:\n",
    "    normalized_corpus_body = clean(df.selftext.to_list())\n",
    "    df.selftext = normalized_corpus_body\n",
    "\n",
    "    normalized_corpus_title = clean(df.title.to_list())\n",
    "    df.title = normalized_corpus_title\n",
    "''''''\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "total_time = (end - start)/60 # minutes\n",
    "print(\"Cleaning Time:\")\n",
    "print(str(total_time) + \" min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving clean text on db\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine(\"sqlite:///db/reddit_financial.db\")\n",
    "\n",
    "for df in dataframes:\n",
    "    df.to_sql('reddit_clean_raw', con=engine, if_exists='replace')"
   ]
  },
  {
   "source": [
    "## Sentiment Analisys\n",
    "We are going to use the AFINN library to analyze the sentiment in both body and title. This library was trained using microblogs which are similar to Reddit in their behavior."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from afinn import Afinn\n",
    "from sqlalchemy import create_engine,text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading posts from the cleaned dataset\n",
    "engine = create_engine(\"sqlite:///db/reddit_financial.db\")\n",
    "df_Clean = pd.read_sql_table(\n",
    "    \"reddit_clean_raw\",\n",
    "    con=engine,\n",
    "    parse_dates=[\n",
    "        'created',\n",
    "        'retrieved',\n",
    "        'edited',\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   index      id   author             created           retrieved  \\\n",
       "0      5  krnthg  nicky94 2021-01-06 13:25:49 2021-02-28 16:51:20   \n",
       "\n",
       "               edited  pinned  archived  locked  removed  ...  upvote_ratio  \\\n",
       "0 2021-01-06 13:28:54       0         0       0        0  ...          0.94   \n",
       "\n",
       "   score  gilded  total_awards_received num_comments num_crossposts  \\\n",
       "0     14       0                      0            9              0   \n",
       "\n",
       "                                            selftext  thumbnail  \\\n",
       "0  speculation idea gamestop might present overal...       self   \n",
       "\n",
       "                shortlink  subReddit  \n",
       "0  https://redd.it/krnthg        gme  \n",
       "\n",
       "[1 rows x 26 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>id</th>\n      <th>author</th>\n      <th>created</th>\n      <th>retrieved</th>\n      <th>edited</th>\n      <th>pinned</th>\n      <th>archived</th>\n      <th>locked</th>\n      <th>removed</th>\n      <th>...</th>\n      <th>upvote_ratio</th>\n      <th>score</th>\n      <th>gilded</th>\n      <th>total_awards_received</th>\n      <th>num_comments</th>\n      <th>num_crossposts</th>\n      <th>selftext</th>\n      <th>thumbnail</th>\n      <th>shortlink</th>\n      <th>subReddit</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5</td>\n      <td>krnthg</td>\n      <td>nicky94</td>\n      <td>2021-01-06 13:25:49</td>\n      <td>2021-02-28 16:51:20</td>\n      <td>2021-01-06 13:28:54</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.94</td>\n      <td>14</td>\n      <td>0</td>\n      <td>0</td>\n      <td>9</td>\n      <td>0</td>\n      <td>speculation idea gamestop might present overal...</td>\n      <td>self</td>\n      <td>https://redd.it/krnthg</td>\n      <td>gme</td>\n    </tr>\n  </tbody>\n</table>\n<p>1 rows Ã— 26 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 191
    }
   ],
   "source": [
    "df_Clean.shape\n",
    "df_Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using AFINN\n",
    "afinn = Afinn()\n",
    "sentiment_afinn_body = []\n",
    "sentiment_afinn_class_body = []\n",
    "\n",
    "sentiment_afinn_title = []\n",
    "sentiment_afinn_class_title= []\n",
    "\n",
    "for row in df_Clean.selftext:\n",
    "    afinn_score = afinn.score(row)\n",
    "    sentiment_afinn_body.append(afinn_score)\n",
    "    sentiment_afinn_class_body.append('positive' if afinn_score>0 else 'neutral' if afinn_score == 0 else 'negative')\n",
    "\n",
    "\n",
    "for row in df_Clean.title:\n",
    "    afinn_score = afinn.score(row)\n",
    "    sentiment_afinn_title.append(afinn_score)\n",
    "    sentiment_afinn_class_title.append('positive' if afinn_score>0 else 'neutral' if afinn_score == 0 else 'negative')\n",
    "\n",
    "\n",
    "df_Clean['SentimentAFINN_body'] = sentiment_afinn_body \n",
    "df_Clean['SentimentAFINN_class_body'] = sentiment_afinn_class_body \n",
    "df_Clean['SentimentAFINN_title'] = sentiment_afinn_title \n",
    "df_Clean['SentimentAFINN_class_title'] = sentiment_afinn_class_title "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['index', 'id', 'author', 'created', 'retrieved', 'edited', 'pinned',\n",
       "       'archived', 'locked', 'removed', 'deleted', 'is_self', 'is_video',\n",
       "       'is_original_content', 'title', 'link_flair_text', 'upvote_ratio',\n",
       "       'score', 'gilded', 'total_awards_received', 'num_comments',\n",
       "       'num_crossposts', 'selftext', 'thumbnail', 'shortlink', 'subReddit',\n",
       "       'SentimentAFINN_body', 'SentimentAFINN_class_body',\n",
       "       'SentimentAFINN_title', 'SentimentAFINN_class_title'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 193
    }
   ],
   "source": [
    "df_Clean.head()\n",
    "df_Clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Putting data on a temp table\n",
    "df_Clean.to_sql('reddit_data_temp', con=engine, if_exists='replace',index=False)\n",
    "\n",
    "#Adding new IDs to main table\n",
    "connection = engine.connect()\n",
    "result = connection.execute(\"INSERT OR IGNORE INTO reddit_data SELECT id, author, created, retrieved, edited, pinned, archived, locked, removed, deleted, is_self, is_video, is_original_content, title, link_flair_text, upvote_ratio, score, gilded, total_awards_received, num_comments, num_crossposts, selftext, thumbnail, shortlink, subReddit, SentimentAFINN_body, SentimentAFINN_class_body, SentimentAFINN_title, SentimentAFINN_class_title FROM reddit_data_temp\")\n",
    "connection.close()\n",
    "\n",
    "#Deleting temp table\n",
    "connection = engine.connect()\n",
    "result = connection.execute(\"DROP TABLE reddit_data_temp\")\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = engine.connect()\n",
    "result = connection.execute(\"DELETE FROM reddit_clean_raw WHERE id IN (SELECT id from reddit_data)\")\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}